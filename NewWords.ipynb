{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordGeneration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import io"
      ],
      "metadata": {
        "id": "X8xxt53665BQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "OIkVkfb-USl9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading training data and doing some setup\n",
        "adjs_df = pd.read_csv('Adjectives.csv',  usecols=[1], header = 0, delimiter=\",\", quoting=csv.QUOTE_NONE, \n",
        "                       encoding='utf-8')\n",
        "adjs_df.columns = ['word']\n",
        "\n",
        "\n",
        "adjs = adjs_df.word.tolist()\n",
        "adjs = [i.replace(\" \", \"\") for i in adjs]\n",
        "text = \" \".join(adjs).lower().replace(\"\\\"\", \"\")\n",
        "print(text[:100], len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "# print(chars)\n",
        "\n",
        "# making dicts for encoding of characters\n",
        "char_indices = dict((v, c) for c,v in enumerate(chars))\n",
        "indices_char = dict((c, v) for c,v in enumerate(chars))\n",
        "\n",
        "maxlen = 30\n",
        "step = 3\n",
        "sequences = []\n",
        "next_char = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "  sequences.append(text[i : i + maxlen])\n",
        "  next_char.append(text[i + maxlen])\n",
        "\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "# \n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_char[i]]] = 1\n"
      ],
      "metadata": {
        "id": "1hlTgEuS65Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "# try adam optimizer aswell\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "\n",
        "model.save('adj_generate_model.h5')"
      ],
      "metadata": {
        "id": "IQaQfsStTwwC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "6h6TU-TtX8yX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.fit(x, y, batch_size=batch_size, epochs=1)\n",
        "    print(\"Epoch: \" + str(epoch))\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        generated = \"\"\n",
        "        sentence = text[start_index : start_index + maxlen]\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "            sentence = sentence[1:] + next_char\n",
        "            generated += next_char\n",
        "        # print(\"Generated: \", generated)\n",
        "\n",
        "        with open(\"generated_adjs.txt\", \"a\") as txt:\n",
        "          # tend to get a lot of repeats for some seeds, filter out when reading back the file\n",
        "          txt.write(generated + \" \")\n",
        "        "
      ],
      "metadata": {
        "id": "7MknLQEWYH8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_words = open('generated_adjs.txt', 'r')\n",
        "generated_words = generated_words.read()\n",
        "\n",
        "words_list = list([i for i in set(generated_words.split(\" \")) if len(i) > 0])\n",
        "\n",
        "print(words_list)"
      ],
      "metadata": {
        "id": "FKUG4YMbVF4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "definitions_dict = {}\n",
        "\n",
        "for word in words_list:\n",
        "  definition = input(\"What does {} mean again?\\n\".format(word))\n",
        "  if definition == \"exit\":\n",
        "    break\n",
        "  if definition == \"skip\":\n",
        "    continue\n",
        "  else:\n",
        "    definitions_dict[word] = [definition]\n",
        "    example = input(\"Right, and what is an example of {} in a sentence?\\n\".format(word))\n",
        "    definitions_dict[word].append(example)"
      ],
      "metadata": {
        "id": "0Pj4Fs34YIeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(definitions_dict)"
      ],
      "metadata": {
        "id": "Jjaro94KYuBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to full dictionary entry\n",
        "\n",
        "# makes a fake phonetic version with direct substitution of letters \n",
        "def make_phonetic(word):\n",
        "  letters_phon = \"æɓçɖɘɸɡʜɪjkɭɰɲøpqʁstʊʋwχyʒ\"\n",
        "  letters_alph = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "  replaced = [letters_phon[letters_alph.index(i)] for i in word]\n",
        "  return \"/{}/\".format(\"\".join(replaced))\n",
        "\n",
        "# puts everything together and appends entry to text file\n",
        "def make_entries():\n",
        "  for key, value in definitions_dict.items():\n",
        "    entry = key + \"\\n adjective \\n\" + make_phonetic(key) + \"\\n\" + value[0] + \"\\n\" + \"- {}\".format(value[1])\n",
        "\n",
        "    with open(\"new_dictionary.txt\", \"a\") as txt:\n",
        "            txt.write(entry + \"\\n\\n\")"
      ],
      "metadata": {
        "id": "534rHdnve5if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_entries()"
      ],
      "metadata": {
        "id": "RMDyJIH6l06B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}